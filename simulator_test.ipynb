{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import numpy as np\n",
    "from src.simulator import Simulator\n",
    "from src.agent import NCPAgent\n",
    "from src.model import Model, Trainer\n",
    "\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "sys.path.append(\"CARLA_SIM/PythonAPI/carla/\")\n",
    "from agents.navigation.basic_agent import BasicAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import Model, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Simulator(world_name='Town03_opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# simulator.world.get_spectator().set_transform(\n",
    "#     carla.Transform(\n",
    "#         location=carla.Location(x=398.7934265136719,\n",
    "#                                 y=-56.03200912475586,\n",
    "#                                 z=3.37939715385437)))\n",
    "\n",
    "# simulator.spawn_car_with_camera(\n",
    "#     rel_coordinates=carla.Location(x=1.2, z=1.9) # camera coords\n",
    "# )\n",
    "# vehicle = simulator.get_vehicle()\n",
    "\n",
    "\n",
    "# output_size = 1\n",
    "# units = 19\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(device)\n",
    "\n",
    "# ncp = Model(output_size, units)\n",
    "# ncp.to(device)\n",
    "# if not os.path.isdir(f'out/{simulator.world_name}'):\n",
    "#     os.mkdir(f'out/{simulator.world_name}')\n",
    "# with open(f'out/{simulator.world_name}/data.txt', 'a+') as f:\n",
    "#     f.write(f'timestamp start = {time.time()}\\n')\n",
    "# agent = NCPAgent(simulator, ncp, target_speed=10)\n",
    "\n",
    "# next_waypoint  = [simulator.world.get_map().get_waypoint(vehicle.get_location(),\n",
    "#                                                     project_to_road=True,\n",
    "#                                                     lane_type=(carla.LaneType.Driving))]\n",
    "\n",
    "# waypoints = []\n",
    "# dist_between_waypoints = 20\n",
    "# waypoint_num = 350\n",
    "# # waypoint_num = 10\n",
    "# for _ in range(waypoint_num):\n",
    "#     waypoints.append(next_waypoint[-1])\n",
    "#     # simulator.world.get_spectator().set_transform(next_waypoint[-1].transform)\n",
    "#     next_waypoint = next_waypoint[-1].next(dist_between_waypoints)\n",
    "\n",
    "# dest_idx = 2\n",
    "# dest = waypoints[dest_idx].transform.location\n",
    "# agent.set_destination(dest)\n",
    "# agent.set_target_speed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # os.rmdir('./out/')\n",
    "\n",
    "# idx = 1\n",
    "# waypoint = waypoints[idx]\n",
    "# to_PIL = torchvision.transforms.ToPILImage()\n",
    "\n",
    "# loss_func = torch.nn.functional.mse_loss\n",
    "# optimizer = torch.optim.Adam(ncp.parameters(), lr=0.001)\n",
    "\n",
    "# # basic_agent = BasicAgent(vehicle=vehicle)\n",
    "# trainer = Trainer(ncp, loss_func, optimizer)\n",
    "\n",
    "# while True:\n",
    "#     # if vehicle.get_location().distance(dest) < 0.5:\n",
    "#     #     print(f\"Destination reached\")\n",
    "#     #     break\n",
    "#     # if (vehicle.get_location().distance(waypoint.transform.location) <= dist_between_waypoints / 4) or \\\n",
    "#     #     (vehicle.get_location().distance(waypoint.transform.location) >= dist_between_waypoints * 2 and \\\n",
    "#     #      vehicle.get_location().distance(waypoint.transform.location) <= dist_between_waypoints * 3):\n",
    "#     #     print(f'Waypoint {idx} was reached')\n",
    "#     #     waypoint = waypoints[idx + 1]\n",
    "#     #     idx += 1\n",
    "    \n",
    "#     control, movement, raw_data, out_tensor = agent.run_step()\n",
    "#     # if raw_data is not None:\n",
    "#     #     trainer.train(raw_data, torch.tensor(control.steer))\n",
    "#     #     print(out_tensor)\n",
    "#     vehicle.apply_control(control)\n",
    "#     if agent.simulator.image_frame is not None:\n",
    "#         with open(f'out/{simulator.world_name}/data.txt', 'a+') as f:\n",
    "#             f.write(f'{agent.simulator.image_frame} : {control.steer}\\n')\n",
    "\n",
    "    \n",
    "#     if agent.done():\n",
    "#         if dest_idx < waypoint_num - 1:\n",
    "#             dest_idx += 10\n",
    "#             dest_idx = min(dest_idx, waypoint_num - 1)\n",
    "#             print(f'Intermediate destination reached. Moving to waypoint {dest_idx}')\n",
    "#             agent.is_done = False\n",
    "#             agent.set_destination(waypoints[dest_idx].transform.location)\n",
    "#             continue\n",
    "\n",
    "#         print(\"The target has been reached, stopping the simulation\")\n",
    "#         break\n",
    "# vehicle.apply_control(carla.VehicleControl(throttle = 0.0, brake=1.0, steer = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulator.destroy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncp.save_model(f'./model/pretrained_wp{waypoint_num}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncp.load_state_dict(torch.load(f'model/pretrained_wp{waypoint_num}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ncp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_waypoint  = [simulator.world.get_map().get_waypoint(vehicle.get_location(),\n",
    "#                                                     project_to_road=True,\n",
    "#                                                     lane_type=(carla.LaneType.Driving))]\n",
    "\n",
    "# waypoints = []\n",
    "# dist_between_waypoints = 20\n",
    "# waypoint_num = 5\n",
    "# for _ in range(waypoint_num):\n",
    "#     waypoints.append(next_waypoint[-1])\n",
    "#     # simulator.world.get_spectator().set_transform(next_waypoint[-1].transform)\n",
    "#     next_waypoint = next_waypoint[-1].next(dist_between_waypoints)\n",
    "\n",
    "# dest_idx = 2\n",
    "# dest = waypoints[dest_idx].transform.location\n",
    "# agent.set_destination(dest)\n",
    "# agent.set_target_speed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1\n",
    "# waypoint = waypoints[idx]\n",
    "# to_PIL = torchvision.transforms.ToPILImage()\n",
    "\n",
    "# loss_func = torch.nn.functional.mse_loss\n",
    "# optimizer = torch.optim.Adam(ncp.parameters(), lr=0.001)\n",
    "\n",
    "# # basic_agent = BasicAgent(vehicle=vehicle)\n",
    "# # trainer = Trainer(ncp, loss_func, optimizer)\n",
    "\n",
    "# while True:\n",
    "#     # if vehicle.get_location().distance(dest) < 0.5:\n",
    "#     #     print(f\"Destination reached\")\n",
    "#     #     break\n",
    "#     # if (vehicle.get_location().distance(waypoint.transform.location) <= dist_between_waypoints / 4) or \\\n",
    "#     #     (vehicle.get_location().distance(waypoint.transform.location) >= dist_between_waypoints * 2 and \\\n",
    "#     #      vehicle.get_location().distance(waypoint.transform.location) <= dist_between_waypoints * 3):\n",
    "#     #     print(f'Waypoint {idx} was reached')\n",
    "#     #     waypoint = waypoints[idx + 1]\n",
    "#     #     idx += 1\n",
    "    \n",
    "#     control, movement, raw_data, out_tensor = agent.run_step()\n",
    "#     # trainer.train(raw_data, torch.tensor(control.steer))\n",
    "#     print(control.steer, movement.item(), out_tensor)\n",
    "#     new_control = carla.VehicleControl(steer=movement.item(), throttle=control.throttle, brake=control.brake)\n",
    "#     vehicle.apply_control(new_control)\n",
    "    \n",
    "#     if agent.done():\n",
    "#         if dest_idx < waypoint_num - 1:\n",
    "#             dest_idx += 10\n",
    "#             dest_idx = min(dest_idx, waypoint_num - 1)\n",
    "#             print(f'Intermediate destination reached. Moving to waypoint {dest_idx}')\n",
    "#             agent.is_done = False\n",
    "#             agent.set_destination(waypoints[dest_idx].transform.location)\n",
    "#             continue\n",
    "\n",
    "#         print(\"The target has been reached, stopping the simulation\")\n",
    "#         break\n",
    "# vehicle.apply_control(carla.VehicleControl(throttle = 0.0, brake=1.0, steer = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulator.destroy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we will save the conv layer weights in this list\n",
    "# model_weights =[]\n",
    "# #we will save the 49 conv layers in this list\n",
    "# conv_layers = []\n",
    "# # get all the model children as list\n",
    "# model_children = list(model.children())\n",
    "# #counter to keep count of the conv layers\n",
    "# counter = 0\n",
    "# #append all the conv layers and their respective wights to the list\n",
    "# for i in range(len(model_children)):\n",
    "#     if type(model_children[i]) == nn.Conv2d:\n",
    "#         counter+=1\n",
    "#         model_weights.append(model_children[i].weight)\n",
    "#         conv_layers.append(model_children[i])\n",
    "#     elif type(model_children[i]) == nn.Sequential:\n",
    "#         for j in range(len(model_children[i])):\n",
    "#             for child in model_children[i][j].children():\n",
    "#                 if type(child) == nn.Conv2d:\n",
    "#                     counter+=1\n",
    "#                     model_weights.append(child.weight)\n",
    "#                     conv_layers.append(child)\n",
    "# print(f\"Total convolution layers: {counter}\")\n",
    "# print(f\"{conv_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = transform(image)\n",
    "# image = image[:3,:,:]\n",
    "# print(f\"Image shape before: {image.shape}\")\n",
    "# image = image.unsqueeze(0)\n",
    "# print(f\"Image shape after: {image.shape}\")\n",
    "# image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = []\n",
    "# names = []\n",
    "# for layer in conv_layers[0:]:\n",
    "#     image = layer(image)\n",
    "#     outputs.append(image)\n",
    "#     names.append(str(layer))\n",
    "# print(len(outputs))\n",
    "# # print feature_maps\n",
    "# for feature_map in outputs:\n",
    "#     print(feature_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed = []\n",
    "# for feature_map in outputs:\n",
    "#     feature_map = feature_map.squeeze(0)\n",
    "#     gray_scale = torch.sum(feature_map,0)\n",
    "#     gray_scale = gray_scale / feature_map.shape[0]\n",
    "#     processed.append(gray_scale.data.cpu().numpy())\n",
    "# for fm in processed:\n",
    "#     print(fm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(30, 50))\n",
    "# for i in range(len(processed)):\n",
    "#     a = fig.add_subplot(5, 4, i+1)\n",
    "#     imgplot = plt.imshow(processed[i])\n",
    "#     a.axis(\"off\")\n",
    "#     a.set_title(names[i].split('(')[0], fontsize=30)\n",
    "# plt.savefig(str('feature_maps.jpg'), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import pytorch_lightning as pl\n",
    "# import torch.utils.data as data\n",
    "\n",
    "# from src.model import SequenceLearner, NCP\n",
    "\n",
    "# ncp = NCP(image_features.shape[1], output_size, units)\n",
    "# ncp_model = ncp.get_ncp()\n",
    "# learner = SequenceLearner(ncp_model, lr=0.01)\n",
    "# trainer = pl.Trainer(\n",
    "#     logger=pl.loggers.CSVLogger(\"log\"),\n",
    "#     max_epochs=5,\n",
    "#     gradient_clip_val=1, #clip grad for training stabilizing\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob, os, sys\n",
    "# # sys.path.append(glob.glob('CARLA_SIM/PythonAPI/carla/'))\n",
    "# sys.path.append(\"CARLA_SIM/PythonAPI/carla/\")\n",
    "# from agents.navigation.basic_agent import BasicAgent\n",
    "# # from agents.navigation.behavior_agent import BehaviorAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulator.spawn_car_with_camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicle = simulator.get_vehicle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # agent = BehaviorAgent(vehicle, behavior='aggressive')\n",
    "# agent = BasicAgent(vehicle)\n",
    "# dest = simulator.spawn_points[50].location\n",
    "# agent.set_destination(dest)\n",
    "# agent.follow_speed_limits(False)\n",
    "# agent.set_target_speed(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# while True:\n",
    "#     if agent.done():\n",
    "#         print(\"The target has been reached, stopping the simulation\")\n",
    "#         break\n",
    "#     # if collided == True:\n",
    "#     #     vehicle.apply_control(carla.VehicleControl(throttle = 0.0, brake=1.0, steer = 0.0))\n",
    "#     #     print(\"Collision detected. Abort\")\n",
    "#     #     break\n",
    "\n",
    "#     vehicle.apply_control(agent.run_step())\n",
    "#     # time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulator.destroy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import TrainingDataset\n",
    "\n",
    "td = TrainingDataset(annotations_file='out/Town01_opt/data.txt',\n",
    "                     img_dir='out/Town01_opt')\n",
    "# photos = td.labels[0].unique()\n",
    "# photo_and_angle = {}\n",
    "# for photo in photos:\n",
    "#     td.labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alloc!\n"
     ]
    }
   ],
   "source": [
    "from src.model import Trainer, Model\n",
    "import torch\n",
    "\n",
    "ncp = Model(4, 19)\n",
    "loss_func = torch.nn.functional.mse_loss\n",
    "optimizer = torch.optim.Adam(ncp.parameters(), lr=0.001)\n",
    "\n",
    "tr = Trainer(model=ncp,\n",
    "             loss_func=loss_func,\n",
    "             optimizer=optimizer,\n",
    "             annotations_file='out/Town01_opt/data.txt',\n",
    "             img_dir='out/Town01_opt',\n",
    "             test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20392 [00:00<?, ?it/s]/mnt/hard_drive/habkaffee_part/programming/coursework/src/model.py:131: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_func(pred_angle[0], true_angle)\n",
      "/home/habkaffee/hard_drive/programming/coursework/venv/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 27%|██▋       | 5591/20392 [00:29<01:18, 189.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/hard_drive/habkaffee_part/programming/coursework/src/model.py:152\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, batch_size)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 152\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m running_vlos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/mnt/hard_drive/habkaffee_part/programming/coursework/src/model.py:128\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[0;34m(self, epoch, logger, batch_size)\u001b[0m\n\u001b[1;32m    124\u001b[0m true_angle \u001b[38;5;241m=\u001b[39m true_angle\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# image = pil_to_tensor(image)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# image = torch.from_numpy(image)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# print(f'Img={image}, angl = {true_angle}')\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m pred_angle, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# print(pred_angle)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# print(pred_angle[0].shape)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(pred_angle[\u001b[38;5;241m0\u001b[39m], true_angle)\n",
      "File \u001b[0;32m~/hard_drive/programming/coursework/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hard_drive/programming/coursework/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/hard_drive/habkaffee_part/programming/coursework/src/model.py:52\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, input, hx, timespans)\u001b[0m\n\u001b[1;32m     50\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# features.to(self.device)\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m predicted_angle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimespans\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_angle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhx\n",
      "File \u001b[0;32m~/hard_drive/programming/coursework/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hard_drive/programming/coursework/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/hard_drive/programming/coursework/venv/lib/python3.8/site-packages/ncps/torch/ltc.py:185\u001b[0m, in \u001b[0;36mLTC.forward\u001b[0;34m(self, input, hx, timespans)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_mixed:\n\u001b[1;32m    184\u001b[0m     h_state, c_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(inputs, (h_state, c_state))\n\u001b[0;32m--> 185\u001b[0m h_out, h_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_cell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_sequences:\n\u001b[1;32m    187\u001b[0m     output_sequence\u001b[38;5;241m.\u001b[39mappend(h_out)\n",
      "File \u001b[0;32m~/hard_drive/programming/coursework/venv/lib/python3.8/site-packages/ncps/torch/ltc_cell.py:283\u001b[0m, in \u001b[0;36mLTCCell.forward\u001b[0;34m(self, inputs, states, elapsed_time)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, states, elapsed_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# Regularly sampled mode (elapsed time = 1 second)\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_inputs(inputs)\n\u001b[0;32m--> 283\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ode_solver\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melapsed_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_outputs(next_state)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs, next_state\n",
      "File \u001b[0;32m~/hard_drive/programming/coursework/venv/lib/python3.8/site-packages/ncps/torch/ltc_cell.py:210\u001b[0m, in \u001b[0;36mLTCCell._ode_solver\u001b[0;34m(self, inputs, state, elapsed_time)\u001b[0m\n\u001b[1;32m    205\u001b[0m v_pre \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# We can pre-compute the effects of the sensory neurons here\u001b[39;00m\n\u001b[1;32m    208\u001b[0m sensory_w_activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_positive_fn(\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensory_w\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 210\u001b[0m ) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sigmoid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msensory_mu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msensory_sigma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m sensory_w_activation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    214\u001b[0m     sensory_w_activation \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensory_sparsity_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    215\u001b[0m )\n\u001b[1;32m    217\u001b[0m sensory_rev_activation \u001b[38;5;241m=\u001b[39m sensory_w_activation \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensory_erev\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/hard_drive/programming/coursework/venv/lib/python3.8/site-packages/ncps/torch/ltc_cell.py:200\u001b[0m, in \u001b[0;36mLTCCell._sigmoid\u001b[0;34m(self, v_pre, mu, sigma)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sigmoid\u001b[39m(\u001b[38;5;28mself\u001b[39m, v_pre, mu, sigma):\n\u001b[1;32m    199\u001b[0m     v_pre \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(v_pre, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# For broadcasting\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     mues \u001b[38;5;241m=\u001b[39m \u001b[43mv_pre\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\n\u001b[1;32m    201\u001b[0m     x \u001b[38;5;241m=\u001b[39m sigma \u001b[38;5;241m*\u001b[39m mues\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr.train(epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
