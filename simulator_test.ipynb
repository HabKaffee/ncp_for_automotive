{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import numpy as np\n",
    "from src.simulator import Simulator\n",
    "from src.agent import NCPAgent\n",
    "from src.model import Model, Trainer\n",
    "\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "sys.path.append(\"CARLA_SIM/PythonAPI/carla/\")\n",
    "from agents.navigation.basic_agent import BasicAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import Model, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Simulator(world_name='Town03_opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "alloc!\n",
      "Camera stream started\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "simulator.world.get_spectator().set_transform(\n",
    "    carla.Transform(\n",
    "        location=carla.Location(x=398.7934265136719,\n",
    "                                y=-56.03200912475586,\n",
    "                                z=3.37939715385437)))\n",
    "\n",
    "simulator.spawn_car_with_camera(\n",
    "    rel_coordinates=carla.Location(x=1.2, z=1.9) # camera coords\n",
    ")\n",
    "vehicle = simulator.get_vehicle()\n",
    "\n",
    "\n",
    "output_size = 1\n",
    "units = 19\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "ncp = Model(output_size, units)\n",
    "ncp.to(device)\n",
    "if not os.path.isdir(f'out/{simulator.world_name}'):\n",
    "    os.mkdir(f'out/{simulator.world_name}')\n",
    "with open(f'out/{simulator.world_name}/data.txt', 'a+') as f:\n",
    "    f.write(f'timestamp start = {time.time()}\\n')\n",
    "agent = NCPAgent(simulator, ncp, target_speed=10)\n",
    "\n",
    "next_waypoint  = [simulator.world.get_map().get_waypoint(vehicle.get_location(),\n",
    "                                                    project_to_road=True,\n",
    "                                                    lane_type=(carla.LaneType.Driving))]\n",
    "\n",
    "waypoints = []\n",
    "dist_between_waypoints = 20\n",
    "waypoint_num = 350\n",
    "# waypoint_num = 10\n",
    "for _ in range(waypoint_num):\n",
    "    waypoints.append(next_waypoint[-1])\n",
    "    # simulator.world.get_spectator().set_transform(next_waypoint[-1].transform)\n",
    "    next_waypoint = next_waypoint[-1].next(dist_between_waypoints)\n",
    "\n",
    "dest_idx = 2\n",
    "dest = waypoints[dest_idx].transform.location\n",
    "agent.set_destination(dest)\n",
    "agent.set_target_speed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate destination reached. Moving to waypoint 12\n",
      "Intermediate destination reached. Moving to waypoint 22\n",
      "Intermediate destination reached. Moving to waypoint 32\n",
      "Intermediate destination reached. Moving to waypoint 42\n",
      "Intermediate destination reached. Moving to waypoint 52\n",
      "Intermediate destination reached. Moving to waypoint 62\n",
      "Intermediate destination reached. Moving to waypoint 72\n",
      "Intermediate destination reached. Moving to waypoint 82\n",
      "Intermediate destination reached. Moving to waypoint 92\n",
      "Intermediate destination reached. Moving to waypoint 102\n",
      "Intermediate destination reached. Moving to waypoint 112\n",
      "Intermediate destination reached. Moving to waypoint 122\n",
      "Intermediate destination reached. Moving to waypoint 132\n",
      "Intermediate destination reached. Moving to waypoint 142\n",
      "Intermediate destination reached. Moving to waypoint 152\n",
      "Intermediate destination reached. Moving to waypoint 162\n",
      "Intermediate destination reached. Moving to waypoint 172\n",
      "Intermediate destination reached. Moving to waypoint 182\n",
      "Intermediate destination reached. Moving to waypoint 192\n",
      "Intermediate destination reached. Moving to waypoint 202\n",
      "Intermediate destination reached. Moving to waypoint 212\n",
      "Intermediate destination reached. Moving to waypoint 222\n",
      "Intermediate destination reached. Moving to waypoint 232\n",
      "Intermediate destination reached. Moving to waypoint 242\n",
      "Intermediate destination reached. Moving to waypoint 252\n",
      "Intermediate destination reached. Moving to waypoint 262\n",
      "Intermediate destination reached. Moving to waypoint 272\n",
      "Intermediate destination reached. Moving to waypoint 282\n",
      "Intermediate destination reached. Moving to waypoint 292\n",
      "Intermediate destination reached. Moving to waypoint 302\n",
      "Intermediate destination reached. Moving to waypoint 312\n",
      "Intermediate destination reached. Moving to waypoint 322\n",
      "Intermediate destination reached. Moving to waypoint 332\n",
      "Intermediate destination reached. Moving to waypoint 342\n",
      "Intermediate destination reached. Moving to waypoint 349\n",
      "The target has been reached, stopping the simulation\n"
     ]
    }
   ],
   "source": [
    "# os.rmdir('./out/')\n",
    "\n",
    "idx = 1\n",
    "waypoint = waypoints[idx]\n",
    "to_PIL = torchvision.transforms.ToPILImage()\n",
    "\n",
    "loss_func = torch.nn.functional.mse_loss\n",
    "optimizer = torch.optim.Adam(ncp.parameters(), lr=0.001)\n",
    "\n",
    "# basic_agent = BasicAgent(vehicle=vehicle)\n",
    "trainer = Trainer(ncp, loss_func, optimizer)\n",
    "\n",
    "while True:\n",
    "    # if vehicle.get_location().distance(dest) < 0.5:\n",
    "    #     print(f\"Destination reached\")\n",
    "    #     break\n",
    "    # if (vehicle.get_location().distance(waypoint.transform.location) <= dist_between_waypoints / 4) or \\\n",
    "    #     (vehicle.get_location().distance(waypoint.transform.location) >= dist_between_waypoints * 2 and \\\n",
    "    #      vehicle.get_location().distance(waypoint.transform.location) <= dist_between_waypoints * 3):\n",
    "    #     print(f'Waypoint {idx} was reached')\n",
    "    #     waypoint = waypoints[idx + 1]\n",
    "    #     idx += 1\n",
    "    \n",
    "    control, movement, raw_data, out_tensor = agent.run_step()\n",
    "    # if raw_data is not None:\n",
    "    #     trainer.train(raw_data, torch.tensor(control.steer))\n",
    "    #     print(out_tensor)\n",
    "    vehicle.apply_control(control)\n",
    "    if agent.simulator.image_frame is not None:\n",
    "        with open(f'out/{simulator.world_name}/data.txt', 'a+') as f:\n",
    "            f.write(f'{agent.simulator.image_frame} : {control.steer}\\n')\n",
    "\n",
    "    \n",
    "    if agent.done():\n",
    "        if dest_idx < waypoint_num - 1:\n",
    "            dest_idx += 10\n",
    "            dest_idx = min(dest_idx, waypoint_num - 1)\n",
    "            print(f'Intermediate destination reached. Moving to waypoint {dest_idx}')\n",
    "            agent.is_done = False\n",
    "            agent.set_destination(waypoints[dest_idx].transform.location)\n",
    "            continue\n",
    "\n",
    "        print(\"The target has been reached, stopping the simulation\")\n",
    "        break\n",
    "vehicle.apply_control(carla.VehicleControl(throttle = 0.0, brake=1.0, steer = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera stream stopped\n"
     ]
    }
   ],
   "source": [
    "simulator.destroy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncp.save_model(f'./model/pretrained_wp{waypoint_num}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.destroy_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncp.load_state_dict(torch.load(f'model/pretrained_wp{waypoint_num}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ncp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_waypoint  = [simulator.world.get_map().get_waypoint(vehicle.get_location(),\n",
    "                                                    project_to_road=True,\n",
    "                                                    lane_type=(carla.LaneType.Driving))]\n",
    "\n",
    "waypoints = []\n",
    "dist_between_waypoints = 20\n",
    "waypoint_num = 5\n",
    "for _ in range(waypoint_num):\n",
    "    waypoints.append(next_waypoint[-1])\n",
    "    # simulator.world.get_spectator().set_transform(next_waypoint[-1].transform)\n",
    "    next_waypoint = next_waypoint[-1].next(dist_between_waypoints)\n",
    "\n",
    "dest_idx = 2\n",
    "dest = waypoints[dest_idx].transform.location\n",
    "agent.set_destination(dest)\n",
    "agent.set_target_speed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "waypoint = waypoints[idx]\n",
    "to_PIL = torchvision.transforms.ToPILImage()\n",
    "\n",
    "loss_func = torch.nn.functional.mse_loss\n",
    "optimizer = torch.optim.Adam(ncp.parameters(), lr=0.001)\n",
    "\n",
    "# basic_agent = BasicAgent(vehicle=vehicle)\n",
    "# trainer = Trainer(ncp, loss_func, optimizer)\n",
    "\n",
    "while True:\n",
    "    # if vehicle.get_location().distance(dest) < 0.5:\n",
    "    #     print(f\"Destination reached\")\n",
    "    #     break\n",
    "    # if (vehicle.get_location().distance(waypoint.transform.location) <= dist_between_waypoints / 4) or \\\n",
    "    #     (vehicle.get_location().distance(waypoint.transform.location) >= dist_between_waypoints * 2 and \\\n",
    "    #      vehicle.get_location().distance(waypoint.transform.location) <= dist_between_waypoints * 3):\n",
    "    #     print(f'Waypoint {idx} was reached')\n",
    "    #     waypoint = waypoints[idx + 1]\n",
    "    #     idx += 1\n",
    "    \n",
    "    control, movement, raw_data, out_tensor = agent.run_step()\n",
    "    # trainer.train(raw_data, torch.tensor(control.steer))\n",
    "    print(control.steer, movement.item(), out_tensor)\n",
    "    new_control = carla.VehicleControl(steer=movement.item(), throttle=control.throttle, brake=control.brake)\n",
    "    vehicle.apply_control(new_control)\n",
    "    \n",
    "    if agent.done():\n",
    "        if dest_idx < waypoint_num - 1:\n",
    "            dest_idx += 10\n",
    "            dest_idx = min(dest_idx, waypoint_num - 1)\n",
    "            print(f'Intermediate destination reached. Moving to waypoint {dest_idx}')\n",
    "            agent.is_done = False\n",
    "            agent.set_destination(waypoints[dest_idx].transform.location)\n",
    "            continue\n",
    "\n",
    "        print(\"The target has been reached, stopping the simulation\")\n",
    "        break\n",
    "vehicle.apply_control(carla.VehicleControl(throttle = 0.0, brake=1.0, steer = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.destroy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we will save the conv layer weights in this list\n",
    "# model_weights =[]\n",
    "# #we will save the 49 conv layers in this list\n",
    "# conv_layers = []\n",
    "# # get all the model children as list\n",
    "# model_children = list(model.children())\n",
    "# #counter to keep count of the conv layers\n",
    "# counter = 0\n",
    "# #append all the conv layers and their respective wights to the list\n",
    "# for i in range(len(model_children)):\n",
    "#     if type(model_children[i]) == nn.Conv2d:\n",
    "#         counter+=1\n",
    "#         model_weights.append(model_children[i].weight)\n",
    "#         conv_layers.append(model_children[i])\n",
    "#     elif type(model_children[i]) == nn.Sequential:\n",
    "#         for j in range(len(model_children[i])):\n",
    "#             for child in model_children[i][j].children():\n",
    "#                 if type(child) == nn.Conv2d:\n",
    "#                     counter+=1\n",
    "#                     model_weights.append(child.weight)\n",
    "#                     conv_layers.append(child)\n",
    "# print(f\"Total convolution layers: {counter}\")\n",
    "# print(f\"{conv_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = transform(image)\n",
    "# image = image[:3,:,:]\n",
    "# print(f\"Image shape before: {image.shape}\")\n",
    "# image = image.unsqueeze(0)\n",
    "# print(f\"Image shape after: {image.shape}\")\n",
    "# image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = []\n",
    "# names = []\n",
    "# for layer in conv_layers[0:]:\n",
    "#     image = layer(image)\n",
    "#     outputs.append(image)\n",
    "#     names.append(str(layer))\n",
    "# print(len(outputs))\n",
    "# # print feature_maps\n",
    "# for feature_map in outputs:\n",
    "#     print(feature_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed = []\n",
    "# for feature_map in outputs:\n",
    "#     feature_map = feature_map.squeeze(0)\n",
    "#     gray_scale = torch.sum(feature_map,0)\n",
    "#     gray_scale = gray_scale / feature_map.shape[0]\n",
    "#     processed.append(gray_scale.data.cpu().numpy())\n",
    "# for fm in processed:\n",
    "#     print(fm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(30, 50))\n",
    "# for i in range(len(processed)):\n",
    "#     a = fig.add_subplot(5, 4, i+1)\n",
    "#     imgplot = plt.imshow(processed[i])\n",
    "#     a.axis(\"off\")\n",
    "#     a.set_title(names[i].split('(')[0], fontsize=30)\n",
    "# plt.savefig(str('feature_maps.jpg'), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import pytorch_lightning as pl\n",
    "# import torch.utils.data as data\n",
    "\n",
    "# from src.model import SequenceLearner, NCP\n",
    "\n",
    "# ncp = NCP(image_features.shape[1], output_size, units)\n",
    "# ncp_model = ncp.get_ncp()\n",
    "# learner = SequenceLearner(ncp_model, lr=0.01)\n",
    "# trainer = pl.Trainer(\n",
    "#     logger=pl.loggers.CSVLogger(\"log\"),\n",
    "#     max_epochs=5,\n",
    "#     gradient_clip_val=1, #clip grad for training stabilizing\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, sys\n",
    "# sys.path.append(glob.glob('CARLA_SIM/PythonAPI/carla/'))\n",
    "sys.path.append(\"CARLA_SIM/PythonAPI/carla/\")\n",
    "from agents.navigation.basic_agent import BasicAgent\n",
    "# from agents.navigation.behavior_agent import BehaviorAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.spawn_car_with_camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = simulator.get_vehicle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = BehaviorAgent(vehicle, behavior='aggressive')\n",
    "agent = BasicAgent(vehicle)\n",
    "dest = simulator.spawn_points[50].location\n",
    "agent.set_destination(dest)\n",
    "agent.follow_speed_limits(False)\n",
    "agent.set_target_speed(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    if agent.done():\n",
    "        print(\"The target has been reached, stopping the simulation\")\n",
    "        break\n",
    "    # if collided == True:\n",
    "    #     vehicle.apply_control(carla.VehicleControl(throttle = 0.0, brake=1.0, steer = 0.0))\n",
    "    #     print(\"Collision detected. Abort\")\n",
    "    #     break\n",
    "\n",
    "    vehicle.apply_control(agent.run_step())\n",
    "    # time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.destroy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Image  Steer_angle\n",
      "0         1426    -0.274289\n",
      "1         1426    -0.274289\n",
      "2         1426    -0.274289\n",
      "3         1426    -0.274289\n",
      "4         1426    -0.274289\n",
      "...        ...          ...\n",
      "777489  207211     0.000023\n",
      "777490  207211     0.000020\n",
      "777491  207211     0.000022\n",
      "777492  207211     0.000022\n",
      "777493  207212     0.000000\n",
      "\n",
      "[777494 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Steer_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1426</td>\n",
       "      <td>-0.274289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1443</td>\n",
       "      <td>-0.274289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1444</td>\n",
       "      <td>-0.274289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1445</td>\n",
       "      <td>-0.274289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1446</td>\n",
       "      <td>-0.274289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203924</th>\n",
       "      <td>207208</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203925</th>\n",
       "      <td>207209</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203926</th>\n",
       "      <td>207210</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203927</th>\n",
       "      <td>207211</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203928</th>\n",
       "      <td>207212</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203929 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Image  Steer_angle\n",
       "0         1426    -0.274289\n",
       "1         1443    -0.274289\n",
       "2         1444    -0.274289\n",
       "3         1445    -0.274289\n",
       "4         1446    -0.274289\n",
       "...        ...          ...\n",
       "203924  207208     0.000025\n",
       "203925  207209     0.000024\n",
       "203926  207210     0.000022\n",
       "203927  207211     0.000022\n",
       "203928  207212     0.000000\n",
       "\n",
       "[203929 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.model import TrainingDataset\n",
    "\n",
    "td = TrainingDataset(annotations_file='out/Town01_opt/data.txt',\n",
    "                     img_dir='out/Town01_opt')\n",
    "\n",
    "# photos = td.labels[0].unique()\n",
    "# photo_and_angle = {}\n",
    "# for photo in photos:\n",
    "#     td.labels.sum()\n",
    "td_unique = td.labels.groupby(by=['Image']).Steer_angle.mean().reset_index()\n",
    "td_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alloc!\n"
     ]
    }
   ],
   "source": [
    "from src.model import Trainer, Model\n",
    "import torch\n",
    "\n",
    "ncp = Model(3, 19)\n",
    "loss_func = torch.nn.functional.mse_loss\n",
    "optimizer = torch.optim.Adam(ncp.parameters(), lr=0.001)\n",
    "\n",
    "tr = Trainer(model=ncp,\n",
    "             loss_func=loss_func,\n",
    "             optimizer=optimizer,\n",
    "             annotations_file='out/test_data/data.txt',\n",
    "             img_dir='out/test_data',\n",
    "             test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "[(tensor([[[243, 249, 210,  ..., 241, 245, 232],\n",
      "         [231, 238, 203,  ..., 235, 246, 180],\n",
      "         [206, 221, 167,  ..., 217, 209, 196],\n",
      "         ...,\n",
      "         [128, 113, 108,  ..., 114, 119, 134],\n",
      "         [129, 142, 116,  ..., 129, 140, 133],\n",
      "         [104, 113, 109,  ..., 147, 132, 142]],\n",
      "\n",
      "        [[243, 249, 210,  ..., 240, 242, 229],\n",
      "         [231, 238, 203,  ..., 234, 243, 177],\n",
      "         [206, 221, 167,  ..., 215, 206, 193],\n",
      "         ...,\n",
      "         [126, 111, 106,  ..., 112, 116, 131],\n",
      "         [127, 140, 114,  ..., 127, 137, 130],\n",
      "         [102, 111, 107,  ..., 145, 129, 139]],\n",
      "\n",
      "        [[245, 250, 212,  ..., 244, 250, 238],\n",
      "         [233, 240, 205,  ..., 239, 251, 186],\n",
      "         [208, 222, 169,  ..., 221, 215, 202],\n",
      "         ...,\n",
      "         [137, 122, 117,  ..., 123, 127, 142],\n",
      "         [138, 151, 125,  ..., 138, 148, 141],\n",
      "         [113, 122, 118,  ..., 156, 140, 150]]], dtype=torch.uint8), 0.5), (tensor([[[251, 251, 251,  ..., 251, 251, 251],\n",
      "         [251, 251, 251,  ..., 251, 251, 251],\n",
      "         [251, 251, 251,  ..., 251, 251, 251],\n",
      "         ...,\n",
      "         [126, 126, 127,  ..., 135, 134, 134],\n",
      "         [124, 124, 126,  ..., 135, 135, 135],\n",
      "         [123, 124, 124,  ..., 135, 135, 135]],\n",
      "\n",
      "        [[251, 251, 251,  ..., 251, 251, 251],\n",
      "         [251, 251, 251,  ..., 251, 251, 251],\n",
      "         [251, 251, 251,  ..., 251, 251, 251],\n",
      "         ...,\n",
      "         [126, 126, 127,  ..., 135, 134, 134],\n",
      "         [124, 124, 126,  ..., 135, 135, 135],\n",
      "         [123, 124, 124,  ..., 135, 135, 135]],\n",
      "\n",
      "        [[251, 251, 251,  ..., 251, 251, 251],\n",
      "         [251, 251, 251,  ..., 251, 251, 251],\n",
      "         [251, 251, 251,  ..., 251, 251, 251],\n",
      "         ...,\n",
      "         [126, 126, 127,  ..., 137, 136, 136],\n",
      "         [124, 124, 126,  ..., 137, 137, 137],\n",
      "         [123, 124, 124,  ..., 137, 137, 137]]], dtype=torch.uint8), 0.2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 78, 200])\n",
      "torch.Size([1, 3, 37, 98])\n",
      "torch.Size([1, 3, 17, 47])\n",
      "torch.Size([1, 3, 8, 23])\n",
      "torch.Size([1, 3, 6, 21])\n",
      "torch.Size([1, 3, 4, 19])\n",
      "torch.Size([1, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x228 and 76x1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/coursework_ncp/src/model.py:155\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 155\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m running_vlos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/programming/coursework_ncp/src/model.py:133\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[0;34m(self, epoch, logger)\u001b[0m\n\u001b[1;32m    124\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_ds,\n\u001b[1;32m    125\u001b[0m                                  batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m    126\u001b[0m                                  shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m                                  num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, true_angle \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# image = pil_to_tensor(image)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# image = torch.from_numpy(image)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# print(f'Img={image}, angl = {true_angle}')\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     pred_angle, hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# print(pred_angle[0].shape)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(pred_angle, true_angle)\n",
      "File \u001b[0;32m~/programming/coursework_ncp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/coursework_ncp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/coursework_ncp/src/model.py:57\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, input, hx, timespans)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, hx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timespans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 57\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# features.to(self.device)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(features, hx, timespans), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/programming/coursework_ncp/src/model.py:53\u001b[0m, in \u001b[0;36mModel.extract_features\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     48\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# transformed_image = transformed_image.to(self.device)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# x = self.encoder(transformed_image)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# x = self.encoder.segmentation_head(x)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# x = nn.functional.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/coursework_ncp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/coursework_ncp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/coursework_ncp/src/encoder.py:74\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_1(x)\n\u001b[0;32m---> 74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_2(x)\n",
      "File \u001b[0;32m~/programming/coursework_ncp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/coursework_ncp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/coursework_ncp/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x228 and 76x1000)"
     ]
    }
   ],
   "source": [
    "tr.train(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
